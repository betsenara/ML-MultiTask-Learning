{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb34801-ef73-4275-9713-34785e6ed63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1\n",
      "Is CUDA available?: True\n",
      "CUDA device name: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85f0caaf-4166-4795-af6e-d0e5f069baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cb09e-3367-448d-951f-9ae4309a08c5",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b62160-9733-40ca-85f1-5ca6c99195b1",
   "metadata": {},
   "source": [
    "Sentence Transformers generate fixed-length sentence embeddings from variable-length texts. Unlike more complex methods, our implementation utilizes pre-trained BERT models with straightforward pooling techniques such as mean and max pooling to aggregate token information and convert token-level embeddings into sentence-level embeddings.\r\n",
    "\r\n",
    "For classification tasks, sentence-level embeddings effectively categorize sentences. To adapt our Sentence Transformer model for multiple tasks, including Named Entity Recognition (NER), we've incorporated an option to generate token-level embeddings. This is achieved by specifying pooling='no_pool', which bypasses the pooling step and preserves individual token embeddings necessary for tasks requiring detailed granularithod, allowing for dirsaffectively utilize the model for generating sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc931e38-a722-49c8-b97d-12b9a6d0e134",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab82ec-677b-4e42-a36a-599b1994210a",
   "metadata": {},
   "source": [
    "**Initialization:**\n",
    "\n",
    "* model_name: Utilizes a pre-trained BERT model, 'bert-base-uncased'.\n",
    "* max_length: Sets a consistent maximum sequence length for input sentences to ensure uniform processing during tokenization.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "* Processes input_ids and attention_mask generated by the tokenizer.\n",
    "* Performs a forward pass using BERT to obtain the last hidden states.\n",
    "* Depending on the chosen pooling method, aggregates token embeddings into a single vector per sentence.\n",
    "\n",
    "**Pooling Strategies:**\n",
    "\n",
    "* **cls**: Utilizes the CLS tokenâ€™s embedding, primarily for classification tasks.\n",
    "* **mean**: Averages token embeddings to provide a generalized representation of the sentence context.\n",
    "* **max**: Identifies the most significant features by applying max pooling across token embeddings.\n",
    "* **no_pool**: Retains the full sequence of embeddings, suitable for token-level tasks or further processing.\n",
    "\n",
    "**Tokenization and Encoding:**\n",
    "\n",
    "The tokenize_and_encode method handles the tokenization and encoding of raw text inputs, preparing them for the model. It returns embeddings based on the selected pooling strategy, ready for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1da5a5-ae0e-4305-809e-b3f6c46baa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cf49536-b9b3-4150-9ea6-2bfc339ab793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\",max_length=max_length):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pooling='mean'):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        if pooling == \"cls\":\n",
    "            return last_hidden_state[:, 0]\n",
    "        elif pooling == \"mean\":\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand_as(last_hidden_state)\n",
    "            summed = (last_hidden_state * mask_expanded).sum(1)\n",
    "            counts = mask_expanded.sum(1)\n",
    "            return summed / torch.clamp(counts, min=1e-9)\n",
    "        elif pooling == \"max\":\n",
    "            masked_hidden = last_hidden_state.masked_fill(~attention_mask.bool().unsqueeze(-1), float('-inf'))\n",
    "            return torch.max(masked_hidden, 1)[0]\n",
    "        elif pooling == \"no_pool\":\n",
    "            return last_hidden_state\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pooling method specified\")\n",
    "\n",
    "    # Returns the dimension of final hidden layer, to be used in Task 2\n",
    "    def get_hidden_size(self): \n",
    "        return self.bert.config.hidden_size\n",
    "\n",
    "    def tokenize_and_encode(self, sentences, pooling='mean'):\n",
    "        encoded = self.tokenizer(sentences, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=self.max_length)\n",
    "        input_ids = encoded[\"input_ids\"].to(self.bert.device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(self.bert.device)\n",
    "        return self.forward(input_ids, attention_mask, pooling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072c91e-bcc5-4e61-bf02-3e822462042c",
   "metadata": {},
   "source": [
    "**Testing the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7acdf27-0a88-4f45-af48-1fdff03952dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([2, 768])\n",
      "First sentence embedding (first 10 dims): tensor([ 0.0383,  0.1468, -0.0972,  0.0882,  0.2505, -0.5942,  0.2747,  0.7684,\n",
      "        -0.3251, -0.5464], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sentence_transformer_model = SentenceTransformer().to(device)\n",
    "\n",
    "# Example Sentences\n",
    "sentences = [\"Machine learning is amazing!\", \"Artificial intelligence is transforming industries.\"]\n",
    "\n",
    "# Generate Sentence Embeddings with 'mean' pooling\n",
    "embeddings = sentence_transformer_model.tokenize_and_encode(sentences)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  \n",
    "print(\"First sentence embedding (first 10 dims):\", embeddings[0][:10])  # Preview first 10 dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2996402-55b4-4cb9-94d9-756820d7e3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([2, 32, 768])\n",
      "First sentence embedding (first 10 dims): tensor([[ 0.1668,  0.1314, -0.0622,  ..., -0.2837,  0.1851,  0.4111],\n",
      "        [-0.1358,  0.0814, -0.1430,  ..., -0.2873,  0.4565,  0.5274],\n",
      "        [-0.5486,  0.2052, -0.0356,  ..., -1.1440,  0.0069, -0.0233],\n",
      "        ...,\n",
      "        [ 0.3040, -0.1088,  0.4053,  ...,  0.1167,  0.0503, -0.1390],\n",
      "        [ 0.1495, -0.3485,  0.2995,  ...,  0.2205,  0.1133, -0.3069],\n",
      "        [ 0.4380,  0.0248,  0.4735,  ...,  0.0920, -0.0062, -0.0892]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example Sentences\n",
    "sentences = [\"Machine learning is amazing!\", \"Artificial intelligence is transforming industries.\"]\n",
    "\n",
    "# Generate Sentence Embeddings with 'no_pool'\n",
    "embeddings = sentence_transformer_model.tokenize_and_encode(sentences,pooling = 'no_pool')\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # Expected: (2, 768)\n",
    "print(\"First sentence embedding (first 10 dims):\", embeddings[0][:10])  # Preview first 10 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ead2b4-2fd8-4842-8e1a-b0dd13df9a4c",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba95aa-2580-4837-9a80-c40d2b66e9d9",
   "metadata": {},
   "source": [
    "For the task of expanding a Sentence Transformer model to handle multi-task learning, the implementation has been designed to accommodate two distinct natural language processing tasks simultaneously. The architecture changes involve the integration of task-specific heads on top of a shared transformer-based encoder, allowing for both task-specific processing and resource-efficient learning. Here's a concise description of the modifications:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe910e5-f90b-4566-9bb2-d7f730a7cf28",
   "metadata": {},
   "source": [
    "#### Multi-Task Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b56f4d-a747-4d13-ba90-35d9f00fc073",
   "metadata": {},
   "source": [
    "The MultiTaskModel class is structured to utilize the encoding capabilities of a pre-defined transformer model, specifically designed to generate embeddings that serve to multiple tasks. This design ensures that the model can perform different types of analyses on the same input data without redundant processing.\n",
    "\n",
    "**Model Components:**\n",
    "- **Shared Encoder:** The SentenceTransformer model defined in Task 1 is used as the shared backbone. This encoder processes input sentences and produces embeddings. The choice of transformer (model_name) can be specified, with \"bert-base-uncased\" being a common default for English language tasks.\n",
    "\n",
    "- **Task-specific Heads:** Two linear layers serve as task-specific heads:\n",
    "\n",
    "  * **Classifier Head:** This component handles Sentence Classification by predicting the class of a sentence from a predefined set. It utilizes embeddings derived from the pooled output of the sentence transformer. `embedding_dim`, representing the dimensionality of the input features to the classifier, defines the size of the embeddings, while `num_task_A`, specifies the number of categories in the classification task and serves as the output size.\n",
    "  * **NER Head:** For a task such as Named Entity Recognition (NER), which requires token-level predictions. This head operates on the sequence of token embeddings directly output by the transformer without pooling. `num_task_B` represents the number of NER classes, which serve as the output parameter.\n",
    "\n",
    "**Forward Pass:**\n",
    "- **Input Handling:** The model first tokenizes input sentences using the SentenceTransformerâ€™s tokenizer, applying padding and truncation to fit the specified max_length.\n",
    "- **Task-based Processing:**\n",
    "  * For sentence classification, the method selects a pooling strategy (like mean, max, or using the cls token) to condense the entire token sequence into a single vector, which is then fed into the classifier head.\n",
    "  * For NER or similar tasks requiring token-level details, the no_pool strategy is employed, preserving the original sequence of embeddings for token-level classification.\n",
    "\n",
    "This architecture effectively supports multi-task learning by leveraging shared representations for efficiency while allowing for task-specific adjustments via separate heads. This approach not only optimizes the training process but also enhances the model's ability to generalize across different types of tasks by learning shared features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d83450e7-ad7d-4ae3-8caf-0d9fcb9d82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_task_A, num_task_B, device=None, max_length=max_length):\n",
    "        super().__init__()\n",
    "        self.encoder = SentenceTransformer(model_name=model_name)\n",
    "        embedding_dim = self.encoder.get_hidden_size()\n",
    "        self.device = device or 'cpu'\n",
    "        self.classifier_head = nn.Linear(embedding_dim, num_task_A).to(self.device)\n",
    "        self.ner_head = nn.Linear(embedding_dim, num_task_B).to(self.device)\n",
    "        self.encoder.to(self.device)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, sentences, task=\"classifier\", pooling='mean'):\n",
    "        inputs = self.encoder.tokenizer(sentences, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=self.max_length)\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "        if task == 'classifier':\n",
    "            sentence_embeddings = self.encoder(input_ids, attention_mask, pooling=pooling)\n",
    "            logits = self.classifier_head(sentence_embeddings)\n",
    "        elif task == 'ner':\n",
    "            token_level_embeddings = self.encoder(input_ids, attention_mask, pooling='no_pool')\n",
    "            logits = self.ner_head(token_level_embeddings)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown task specified!\")\n",
    "            \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213b548-bc60-4ea6-ab79-b46664e636fe",
   "metadata": {},
   "source": [
    "**Testing the Model**\n",
    "\n",
    "When testing the model, the output logits for the classifier task should be a vector of size `[batch_size, number_of_classes]`, representing the probability distribution across the defined categories for each sentence in the batch. \n",
    "\n",
    "For the NER task, the expected output is `[batch_size, sequence_length, number_of_classes]`. The sequence_length corresponds to the number of tokens in each sentence, which equals the `max_length` due to padding. This ensures consistency in the output dimensions across different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233c411e-002e-4c01-8c56-0e31e073ce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier logits: torch.Size([2, 3])\n",
      "NER logits: torch.Size([2, 32, 5])\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "multi_task_model = MultiTaskModel(\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    num_task_A=3,   # e.g., positive, negative, neutral\n",
    "    num_task_B=5,   # e.g., some entity tags\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "# Classification forward pass\n",
    "sentences_cls = [\"I love this movie!\", \"It was terrible...\"]\n",
    "logits_cls = multi_task_model(sentences_cls, task=\"classifier\")\n",
    "print(\"Classifier logits:\", logits_cls.shape)  \n",
    "\n",
    "# NER forward pass\n",
    "sentences_ner = [\"John lives in New York\", \"Barack Obama was president\"]\n",
    "logits_ner = multi_task_model(sentences_ner, task=\"ner\")\n",
    "print(\"NER logits:\", logits_ner.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff111a-5f1e-481a-ac74-09162999b861",
   "metadata": {},
   "source": [
    "## Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e9850-c292-48c7-81d8-8e11690415ed",
   "metadata": {},
   "source": [
    "Freezing the entire network  or partially freezing have advantages and disadvantages:\n",
    "\n",
    "1. **Freezing the Entire Network:**\n",
    " When the entire network is frozen, weights remain unchanged, and no training is conducted. This approach saves computational resources, as it eliminates the need for updates during training. If the pretrained model's domain is similar to ours, freezing the network can produce effective results, particularly with small datasets, as it helps avoid overfitting. However, this method lacks flexibility and may not perform well unless the pretrained model is extremely close to your domain.\n",
    " \n",
    "2. **Freezing Only Transformer Backbone**\n",
    "The transformer backbone's weights are fixed, allowing the pretrained model to act as a feature extractor. The task-specific heads are trained to map these extracted embeddings to the specific categories or values relevant to our tasks. This method accelerates training compared to fine-tuning the entire model. It is particularly beneficial when dealing with small datasets, as it preserves the core features of the pre-trained model. This approach leverages the generalizability of pre-trained embeddings while customizing the model to specific tasks. Itâ€™s beneficial when the new tasks differ in finer details that the last layers can adapt to. However, if our task diverges significantly from the original training context of the model, performance may degrade.\n",
    "  \n",
    "   \n",
    "3. **Freezing only one of the task-specific heads**\n",
    "   If one of the task-specific heads is frozen, it enables targeted adjustments to the modelâ€™s performance. This strategy is beneficial when one task is well-tuned and stable, while another still requires optimization. By freezing the head of the well-performing task, we can focus training efforts on areas that need improvement, enhancing overall efficiency. This approach is particularly valuable when dealing with tasks of varying complexity and datasets of differing sizes. For example, if a task with limited data has already achieved satisfactory results, freezing its corresponding head prevents overfitting and conserves the integrity of its performance. Simultaneously, it allows continued training for the task with more abundant data, where further performance gains are possible. On the other hand, if one task has significantly more data, training the model jointly without adjustments could lead to dominance by the task with more data, potentially skewing the model's ability to learn from the smaller dataset effectively. Freezing the well-trained task might help mitigate this by allowing more training focus on the underrepresented task. Freezing the head for the more stable or adequately trained task also means fewer parameters require updates during training, which enhances computational efficiency and helps balance the modelâ€™s learning across its various tasks.\n",
    "   ledge. data.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461276e7-4706-4adb-a307-3cf14c76d0b0",
   "metadata": {},
   "source": [
    "#### How to Approach a Transfer Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a6911-eb81-48a0-954b-836af5115f13",
   "metadata": {},
   "source": [
    "1. When selecting a pretrained model, I would prefer a robust option like BERT, which has been extensively trained on diverse and large datasets. Additionally, choosing a model pretrained on data that closely aligns with my specific domain is crucial to leverage transferrable features effectively.\n",
    "2. First we freeze the backbone and unfreeze the task specific layers to adapt the response to the new task. Depending on the performance or overfitting concerns we gradually unfreeze starting from the top layers. use a small learning rate. As I keep unfreezing we would decrease the learning rate.\n",
    "3. When utilizing transfer learning, selecting a pre-trained model that is already trained on vast datasets have several advantages like drastically reducing the need for additional data and shortening training times. However, fine-tuning such a model requires careful consideration.\n",
    "     \n",
    "    * **Gradual Unfreezing:** It's crucial to unfreeze layers progressively to avoid catastrophic forgetting, where the model loses previously learned information from the original training set. We begin with the last few layers, as they are more specialized to the task at hand, and we gradually work our way to earlier layers if necessary.\n",
    "\n",
    "    * **Learning Rate Adjustments:** We start with a smaller learning rate to preserve the core features of the model and prevent the model from deviating too much from its initial training. As we unfreeze more layers, we may slightly increase the learning rate to allow the model to adapt more freely to the specifics of the new task.\n",
    "\n",
    "    * **Preventing Overfitting:** By only updating the weights of the final layers, there's a risk of overfitting, where the model learns the training data too well and performs poorly on unseen data. To combat this, unfreezing additional layers allows the model to learn more generalizable features rather than just memorizing the training data.\n",
    "\n",
    "    * **Adapting to New Domains:** If the new task diverges significantly from the data used in pre-training, it might be beneficial to unfreeze more layers. This approach allows the model to adjust its internal representations more extensively to better suit the new domain.\n",
    "\n",
    "Following these steps can effectively leverage a pre-trained model for new tasks while maintaining a balance between adaptation and retention of learned knowledge.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05195e-29a8-4f82-9a9e-1ca0ea28c4cb",
   "metadata": {},
   "source": [
    "## Task 4: Training Loop Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c93e84-adb3-4850-ab6b-8fc1fedaacf4",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d44d4-45cf-4afe-bacd-5d634f641245",
   "metadata": {},
   "source": [
    "We will generate a simple dataset to test our training implementation and to explore how different properties of the data can be utilized for multi-tasking. This dataset is designed to support tasks that may require multiple labels per instance, with some data points labeled for both tasks and others labeled for only one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fcded-35d8-4688-9636-a3a6f25cdaed",
   "metadata": {},
   "source": [
    "#### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a741d41-bb67-4a77-91f8-3be5a2ad05a3",
   "metadata": {},
   "source": [
    "Our dataset includes:\n",
    "\n",
    "- **Sentences:** Each entry is a string representing a sentence.\n",
    "Sentiment Labels: Numerical labels indicating the sentiment of the sentence:\n",
    "   * 1: Positive\n",
    "   * 0: Neutral\n",
    "   * -1: Negative\n",
    "- **NER Labels:** Lists of integers, where each integer corresponds to a token in the sentence and represents an entity type. If NER labels are not applicable, the entry is None.\n",
    "   * 0: O (No entity)\n",
    "   * 1: PER (Person)\n",
    "   * 2: ORG (Organization)\n",
    "   * 3: LOC (Location)\n",
    "     \n",
    "- **Task Flags:** Descriptors indicating whether labels are provided for both tasks (\"both\") or a single task (\"classification\" for sentiment only, \"ner\" for NER only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f21f1e-3bea-4745-abd7-d49e61e89b5a",
   "metadata": {},
   "source": [
    "#### Data Structure\n",
    "The structured data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "038325bf-dfcc-4699-acb6-35ab3cdc4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating combined data with availability flags\n",
    "data_combined = [\n",
    "    (\"The weather is lovely in California today.\", 1, [0, 0, 0, 0, 3, 0], \"both\"),  # Positive, mentions a location\n",
    "    (\"John ordered pizza from Domino's yesterday.\", 0, [1, 0, 0, 0, 2, 0], \"both\"),  # Neutral, mentions a person and organization\n",
    "    (\"Tesla launches a new model next month.\", 0, [2, 0, 0, 0, 0, 0], \"both\"),  # Neutral, mentions an organization\n",
    "    (\"The movie was great!\", 1, None, \"classification\"),  # Positive, only classification\n",
    "    (\"Julia is now working remotely.\", 0, [1, 0, 0, 0], \"ner\"),  # Neutral, mentions a person\n",
    "    (\"Microsoft introduces Windows 12 in July.\", 0, [2, 0, 0, 3, 0], \"both\"),  # Neutral, mentions an organization and a time\n",
    "    (\"Sadly, the series finale was disappointing.\", -1, None, \"classification\"),  # Negative, only classification\n",
    "    (\"Daniel travels to Spain for a conference.\", 0, [1, 0, 0, 3, 0, 0], \"both\"),  # Neutral, mentions a person and a location\n",
    "    (\"Larry Page resigns from Google.\", -1, [1, 0, 0, 2], \"both\"),  # Negative, mentions people and an organization\n",
    "    (\"The seminar in Boston was extremely informative.\", 1, None, \"classification\")  # Positive, only classification\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba37b9e-94f8-4183-87d9-11ed6651d05d",
   "metadata": {},
   "source": [
    "We will use -2 as a placeholder for missing values to resolve issues encountered with None values in the DataLoader. Additionally, we will represent the negative class in sentiment classification with 2 instead of -1. After making these changes we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "614583bb-1166-444e-9502-f63bf565c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = [\n",
    "    (\"The weather is lovely in California today.\", 1, [0, 0, 0, 0, 3, 0], \"both\"),  # Positive, mentions a location\n",
    "    (\"John ordered pizza from Domino's yesterday.\", 0, [1, 0, 0, 0, 2, 0], \"both\"),  # Neutral, mentions a person and organization\n",
    "    (\"Tesla launches a new model next month.\", 0, [2, 0, 0, 0, 0, 0], \"both\"),  # Neutral, mentions an organization\n",
    "    (\"The movie was great!\", 1, [-2,-2,-2,-2], \"classification\"),  # Positive, only classification\n",
    "    (\"Julia is now working remotely.\", 0, [1, 0, 0, 0], \"ner\"),  # Neutral, mentions a person\n",
    "    (\"Microsoft introduces Windows 12 in July.\", 0, [2, 0, 0, 3, 0], \"both\"),  # Neutral, mentions an organization and a time\n",
    "    (\"Sadly, the series finale was disappointing.\", 2, [-2,-2,-2,-2,-2,-2], \"classification\"),  # Negative, only classification\n",
    "    (\"Daniel travels to Spain for a conference.\", 0, [1, 0, 0, 3, 0, 0], \"both\"),  # Neutral, mentions a person and a location\n",
    "    (\"Larry Page resigns from Google.\", 2, [1, 0, 0, 2], \"both\"),  # Negative, mentions people and an organization\n",
    "    (\"The seminar in Boston was extremely informative.\", 1, [-2,-2,-2,-2,-2,-2,-2], \"classification\")  # Positive, only classification\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ad7a1-7f5b-4fc2-80f1-f5620cb545ba",
   "metadata": {},
   "source": [
    "#### Handling Mixed Task Data in MultiTaskDataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddbe9f-f73f-4b23-ae73-cf87bcf10a82",
   "metadata": {},
   "source": [
    "The MultitaskDataset class is designed for handling datasets that contain mixed tasks like sentence classification and Named Entity Recognition (NER), suitable for integration with PyTorch's DataLoader. Initialized with a dataset and a predefined maximum sequence length, this class prepares data for efficient batch processing critical for training neural networks.\n",
    "\n",
    "In the constructor (`__init__`), it accepts data_combined, a list of tuples containing sentences, classification labels (labelA), NER labels (labelsB), and a task flag (task_available). It ensures that all NER labels are padded to max_length to maintain uniform sequence lengths necessary for models requiring fixed-size inputs.\n",
    "\n",
    "The `__getitem__` method retrieves and processes each data point by ensuring:\n",
    "\n",
    "Classification labels are converted to tensors; if missing, a placeholder value of -2 is used.\n",
    "NER labels are padded with -2 to match max_length if present, or filled entirely with -2 if absent, catering to the fixed-length requirements of certain network architectures.\n",
    "This structured approach ensures every model input batch is consistent, supporting the model's ability to learn effectively from varied training data, especially when some tasks may not be annotated across all dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a69a949a-a9a9-4816-8eef-1ee0d7d1a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskDataset(Dataset):\n",
    "    def __init__(self, data_combined,max_length):\n",
    "        self.data_combined = data_combined\n",
    "        self.max_len_ner = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_combined)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, labelA, labelsB, task_available = self.data_combined[idx]\n",
    "        labelA = torch.tensor([labelA]) if labelA is not None else torch.tensor([-2])  # Handle None for classification labels\n",
    "        if labelsB is not None:\n",
    "            labelsB = torch.tensor(labelsB + [-2] * (self.max_len_ner - len(labelsB)))  # Pad NER labels\n",
    "        else:\n",
    "            labelsB = torch.tensor([-2] * self.max_len_ner)  # Handle completely missing NER labels\n",
    "        return sentence, labelA, labelsB, task_available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151434e-311e-4f1e-9f1b-407ae2c79ca8",
   "metadata": {},
   "source": [
    "#### Handling Mixed-Task Batches in Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702a16a-5535-41c3-9630-4c49a6fb2771",
   "metadata": {},
   "source": [
    "The training implementation employs a batch data loading approach to improve computational efficiency. Given the multi-label nature of our dataset, where each batch may contain data for different tasks, the approach is adapted to handle batches that require specific processing per task label.\n",
    "\n",
    "**Batch Handling**\n",
    "\n",
    "In a standard setup where each task's data is distinct and separate, batching and processing can be straightforward. However, in our multi-task scenario where batches contain mixed labels (both classification and NER), we cannot process the entire batch in a uniform manner. Instead, we iterate over each item within a batch, processing it according to its specific task label. This approach ensures that the model is accurately updated based on the relevant task, preserving task integrity and promoting effective learning across different types of data.\n",
    "\n",
    "**Data Loader and Batch Processing**\n",
    "\n",
    "While a DataLoader is used to manage batch creation efficiently, each batch's mixed nature requires individual processing of data points. This method is particularly suited to our small dataset and helps mitigate the complexities introduced by mixed-task batches.  Consideration for a more streamlined batch processing method could be explored in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9480239c-1afd-4219-8377-a719f73107da",
   "metadata": {},
   "source": [
    "#### Training Process and Loss Function Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39035b96-5f51-4e0e-a4f2-2b85a618784e",
   "metadata": {},
   "source": [
    ".\n",
    "**Loss Function Calculation**\n",
    "\n",
    "During training, we calculate loss separately for each task within a batch, summing these to get a total loss for the batchy:\n",
    "\n",
    "**Classification Task:** We calculate the cross-entropy loss for sentence classification.\n",
    "**NER Task:** Similarly, cross-entropy loss is computed for named entity recognition, with adjustments for sequence length and paddin This adjustment is necessary to handle the token-level details required by NER.\n",
    "\n",
    "**Weighted Loss Considerations**\r\n",
    "\r\n",
    "While the current implementation treats each task equally, introducing weighted loss could optimize training by prioritizing more critical or complex tasks. This adjustment would be particularly useful in scenarios where task importance or data representation varies significantl datasets. performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a3c5a5-1442-454b-bd6f-7c6b2a72a47c",
   "metadata": {},
   "source": [
    "**Batch Processing and Loss Aggregation**\n",
    "\n",
    "Each batch provided by the DataLoader may contain mixed tasks. We process these individually to respect their specific requirements:\n",
    "\n",
    "For classification, the output logits are `[batch_size, num_labels]`.\n",
    "For NER, the output shape is `[batch_size, seq_len, num_labels]`, requiring reshaping for effective loss computation.\n",
    "The losses from both tasks are summed to form a total batch loss, which is then used for backpropagation. This approach ensures that both tasks influence the model's learning, maintaining a balanced focus across different types of data.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "This method facilitates efficient training across various tasks by appropriately managing each task's specific requirements and leveraging shared model features. It aims to enhance the model's adaptability and performance in processing complex, diverse datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3984fe07-e71a-461f-8c67-9a2cd3ffb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multitask(model, dataset, num_epochs=1, batch_size=1):\n",
    "    data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    device = next(model.parameters()).device  # Get the device model is currently on\n",
    "    PAD_INDEX = -2  # Define padding index for NER task, ensure it does not conflict with any label index\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        for sentences, labelsA, labelsBs, task_available in data_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "\n",
    "            # Transfer tensors to the appropriate device\n",
    "            labelsA = labelsA.to(device)\n",
    "            labelsBs = labelsBs.to(device)\n",
    "\n",
    "            #Since each batch contains mixed tasks we go over each element separately\n",
    "            for i, task in enumerate(task_available):\n",
    "                if task in [\"classification\", \"both\"]:\n",
    "                    logits_cls = model(sentences[i], task=\"classifier\", pooling='mean')\n",
    "                    loss_cls = F.cross_entropy(logits_cls, labelsA[i], ignore_index=PAD_INDEX)\n",
    "                    total_loss = total_loss + loss_cls\n",
    "                    print(f\"Task A loss: {loss_cls.item():.4f} for sentence '{sentences[i]}'\")\n",
    "\n",
    "                if task in [\"ner\", \"both\"]:\n",
    "                    logits_ner = model(sentences[i], task=\"ner\", pooling='no_pool')\n",
    "                    batch_size, seq_len, num_labels = logits_ner.shape # Shape of logits \n",
    "                    # Reshape the NER labels to a 1D tensor to match the expected format for loss calculation.\n",
    "                    labelsB_reshaped = labelsBs[i].view(-1)\n",
    "                    # Reshape the NER logits to a 2D tensor where each row represents the logits for a token, facilitating computation of the loss per token.\n",
    "                    logits_ner_reshaped = logits_ner.view(-1, num_labels)\n",
    "                    loss_ner = F.cross_entropy(logits_ner_reshaped, labelsB_reshaped, ignore_index=PAD_INDEX)\n",
    "                    total_loss = total_loss + loss_ner\n",
    "                    print(f\"Task B loss: {loss_ner.item():.4f} for sentence '{sentences[i]}'\")\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355cacd-b39f-431a-b737-b1bf99a9b18a",
   "metadata": {},
   "source": [
    "### Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b330d3d5-879c-4b51-a6d8-26e0073fb410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "model_name = 'bert-base-uncased'\n",
    "num_task_A = 3  # Three sentiment classes (positive, neutral, negative)\n",
    "num_task_B = 4  # Four NER tags (Person, Location, Organization, Other)\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Using GPU if available, otherwise CPU\n",
    "multi_task_model = MultiTaskModel(model_name=model_name, num_task_A=num_task_A, num_task_B=num_task_B, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d45746-2ea8-4da1-ad1b-b1e5f41b8f9d",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877947b3-5118-4d87-b613-5e93e8894146",
   "metadata": {},
   "source": [
    "We will not train the model on real-world data; instead, we will use the small sample dataset provided above to verify the model's functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2884e82-4e06-46a6-bf3c-fc74d225a860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Task A loss: 0.0028 for sentence 'The seminar in Boston was extremely informative.'\n",
      "Task A loss: 0.0133 for sentence 'Larry Page resigns from Google.'\n",
      "Task B loss: 0.1035 for sentence 'Larry Page resigns from Google.'\n",
      "Task A loss: 0.0133 for sentence 'Microsoft introduces Windows 12 in July.'\n",
      "Task B loss: 0.1423 for sentence 'Microsoft introduces Windows 12 in July.'\n",
      "Task A loss: 0.0362 for sentence 'Daniel travels to Spain for a conference.'\n",
      "Task B loss: 0.0716 for sentence 'Daniel travels to Spain for a conference.'\n",
      "Task A loss: 0.0303 for sentence 'The weather is lovely in California today.'\n",
      "Task B loss: 0.1165 for sentence 'The weather is lovely in California today.'\n",
      "Task A loss: 0.0025 for sentence 'Sadly, the series finale was disappointing.'\n",
      "Task A loss: 0.0128 for sentence 'Tesla launches a new model next month.'\n",
      "Task B loss: 0.0958 for sentence 'Tesla launches a new model next month.'\n",
      "Task A loss: 0.0054 for sentence 'The movie was great!'\n",
      "Task B loss: 0.0458 for sentence 'Julia is now working remotely.'\n",
      "Task A loss: 0.0183 for sentence 'John ordered pizza from Domino's yesterday.'\n",
      "Task B loss: 0.0551 for sentence 'John ordered pizza from Domino's yesterday.'\n",
      "Epoch 2\n",
      "Task A loss: 0.0015 for sentence 'The movie was great!'\n",
      "Task A loss: 0.0135 for sentence 'Daniel travels to Spain for a conference.'\n",
      "Task B loss: 0.0209 for sentence 'Daniel travels to Spain for a conference.'\n",
      "Task A loss: 0.1796 for sentence 'Larry Page resigns from Google.'\n",
      "Task B loss: 0.1162 for sentence 'Larry Page resigns from Google.'\n",
      "Task A loss: 0.0088 for sentence 'John ordered pizza from Domino's yesterday.'\n",
      "Task B loss: 0.0297 for sentence 'John ordered pizza from Domino's yesterday.'\n",
      "Task A loss: 0.0010 for sentence 'The seminar in Boston was extremely informative.'\n",
      "Task A loss: 0.0708 for sentence 'Microsoft introduces Windows 12 in July.'\n",
      "Task B loss: 0.0487 for sentence 'Microsoft introduces Windows 12 in July.'\n",
      "Task A loss: 0.0046 for sentence 'Tesla launches a new model next month.'\n",
      "Task B loss: 0.0614 for sentence 'Tesla launches a new model next month.'\n",
      "Task A loss: 0.0033 for sentence 'The weather is lovely in California today.'\n",
      "Task B loss: 0.6640 for sentence 'The weather is lovely in California today.'\n",
      "Task A loss: 0.0015 for sentence 'Sadly, the series finale was disappointing.'\n",
      "Task B loss: 0.0152 for sentence 'Julia is now working remotely.'\n",
      "Epoch 3\n",
      "Task A loss: 0.0042 for sentence 'Daniel travels to Spain for a conference.'\n",
      "Task B loss: 0.0539 for sentence 'Daniel travels to Spain for a conference.'\n",
      "Task A loss: 0.0071 for sentence 'John ordered pizza from Domino's yesterday.'\n",
      "Task B loss: 0.0207 for sentence 'John ordered pizza from Domino's yesterday.'\n",
      "Task A loss: 0.0029 for sentence 'Larry Page resigns from Google.'\n",
      "Task B loss: 0.1601 for sentence 'Larry Page resigns from Google.'\n",
      "Task A loss: 0.0066 for sentence 'Tesla launches a new model next month.'\n",
      "Task B loss: 0.0120 for sentence 'Tesla launches a new model next month.'\n",
      "Task A loss: 0.0065 for sentence 'The seminar in Boston was extremely informative.'\n",
      "Task A loss: 0.0014 for sentence 'Sadly, the series finale was disappointing.'\n",
      "Task A loss: 0.0065 for sentence 'The movie was great!'\n",
      "Task A loss: 0.0052 for sentence 'Microsoft introduces Windows 12 in July.'\n",
      "Task B loss: 0.0788 for sentence 'Microsoft introduces Windows 12 in July.'\n",
      "Task A loss: 0.3940 for sentence 'The weather is lovely in California today.'\n",
      "Task B loss: 0.1062 for sentence 'The weather is lovely in California today.'\n",
      "Task B loss: 0.0110 for sentence 'Julia is now working remotely.'\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "dataset = MultitaskDataset(data_combined,max_length)  \n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 3\n",
    "batch_size = 1 # we chose batch size to be one since it is already handling data one by one in the code (explained above)\n",
    "\n",
    "# Train the model\n",
    "train_multitask(multi_task_model, dataset, num_epochs=num_epochs, batch_size=batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
